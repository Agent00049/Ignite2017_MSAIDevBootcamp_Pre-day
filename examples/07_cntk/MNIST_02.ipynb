{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import gzip\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "from IPython.display import Image\n",
    "\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train data\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Downloading test data\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Done.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Done.\n",
      "Writing train text file...\n",
      "File already exists data\\MNIST\\Train-28x28_cntk_text.txt\n",
      "Writing test text file...\n",
      "File already exists data\\MNIST\\Test-28x28_cntk_text.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Functions to load MNIST images and unpack into train and test set.\n",
    "# - loadData reads image data and formats into a 28x28 long array\n",
    "# - loadLabels reads the corresponding labels data, 1 for each image\n",
    "# - load packs the downloaded image and labels data into a combined format to be read later by \n",
    "#   CNTK text reader \n",
    "\n",
    "def loadData(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x3080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))[0]\n",
    "            if n != cimg:\n",
    "                raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n",
    "            crow = struct.unpack('>I', gz.read(4))[0]\n",
    "            ccol = struct.unpack('>I', gz.read(4))[0]\n",
    "            if crow != 28 or ccol != 28:\n",
    "                raise Exception('Invalid file: expected 28 rows/cols per image.')\n",
    "            # Read data.\n",
    "            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, crow * ccol))\n",
    "\n",
    "def loadLabels(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x1080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))\n",
    "            if n[0] != cimg:\n",
    "                raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n",
    "            # Read labels.\n",
    "            res = np.fromstring(gz.read(cimg), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, 1))\n",
    "\n",
    "def try_download(dataSrc, labelsSrc, cimg):\n",
    "    data = loadData(dataSrc, cimg)\n",
    "    labels = loadLabels(labelsSrc, cimg)\n",
    "    return np.hstack((data, labels))\n",
    "\n",
    "# URLs for the train image and labels data\n",
    "url_train_image = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "url_train_labels = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "num_train_samples = 60000\n",
    "\n",
    "print(\"Downloading train data\")\n",
    "train = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "url_test_image = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "url_test_labels = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "num_test_samples = 10000\n",
    "\n",
    "print(\"Downloading test data\")\n",
    "test = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "\n",
    "# Save the data files into a format compatible with CNTK text reader\n",
    "def savetxt(filename, ndarray):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"Saving\", filename )\n",
    "        with open(filename, 'w') as f:\n",
    "            labels = list(map(' '.join, np.eye(10, dtype=np.uint).astype(str)))\n",
    "            for row in ndarray:\n",
    "                row_str = row.astype(str)\n",
    "                label_str = labels[row[-1]]\n",
    "                feature_str = ' '.join(row_str[:-1])\n",
    "                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "    else:\n",
    "        print(\"File already exists\", filename)\n",
    "\n",
    "# Save the train and test files (prefer our default path for the data)\n",
    "data_dir = os.path.join(\"data\", \"MNIST\")\n",
    "\n",
    "print ('Writing train text file...')\n",
    "savetxt(os.path.join(data_dir, \"Train-28x28_cntk_text.txt\"), train)\n",
    "\n",
    "print ('Writing test text file...')\n",
    "savetxt(os.path.join(data_dir, \"Test-28x28_cntk_text.txt\"), test)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is data\\MNIST\n"
     ]
    }
   ],
   "source": [
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Ensure the training and test data is generated and available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "data_dir = os.path.join(\"data\", \"MNIST\")\n",
    "train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "    data_found = True\n",
    "if not data_found:\n",
    "    raise ValueError(\"Please generate the data\")\n",
    "print(\"Data directory is {0}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Label:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8c788d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a random image\n",
    "sample_number = 5001\n",
    "plt.imshow(train[sample_number,:-1].reshape(28,28), cmap=\"gray_r\")\n",
    "plt.axis('off')\n",
    "print(\"Image Label: \", train[sample_number,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map out the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = C.input_variable((1, 28, 28))\n",
    "label = C.input_variable(10)  # Probabilities for each number (0 - 9)\n",
    "\n",
    "reader_train = create_reader(train_file, True, 784, 10)\n",
    "input_map = {\n",
    "    label  : reader_train.streams.labels,\n",
    "    input  : reader_train.streams.features\n",
    "}\n",
    "\n",
    "with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "    h = input / 255.0\n",
    "    h = C.layers.Convolution2D(filter_shape = (5,5), \n",
    "                               num_filters = 8, \n",
    "                               strides = (2,2), \n",
    "                               pad = True,\n",
    "                               name = 'first_conv')(h)\n",
    "    h = C.layers.Convolution2D(filter_shape = (5,5), \n",
    "                               num_filters = 16, \n",
    "                               strides = (2,2), \n",
    "                               pad = True,\n",
    "                               name = 'second_conv')(h)\n",
    "    z = C.layers.Dense(10, activation = None, name = 'classify')(h)\n",
    "\n",
    "loss = C.cross_entropy_with_softmax(z, label)\n",
    "label_error = C.classification_error(z, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_schedule = C.learning_rate_schedule(0.2, C.UnitType.minibatch)\n",
    "learner = C.sgd(z.parameters, lr_schedule)\n",
    "trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "\n",
    "for i in range(0, int(60000 * 10 / 64)):\n",
    "    data = reader_train.next_minibatch(64, input_map = input_map)\n",
    "    trainer.train_minibatch(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test error: 1.39%\n"
     ]
    }
   ],
   "source": [
    "reader_test = create_reader(test_file, False, 784, 10)\n",
    "test_input_map = {\n",
    "    label  : reader_test.streams.labels,\n",
    "    input  : reader_test.streams.features,\n",
    "}\n",
    "\n",
    "test_result = 0.0\n",
    "for i in range(10000 // 512):\n",
    "    data = reader_test.next_minibatch(512, input_map = test_input_map)\n",
    "    eval_error = trainer.test_minibatch(data)\n",
    "    test_result = test_result + eval_error\n",
    "\n",
    "print(\"Average test error: {0:.2f}%\".format(test_result*100 / (10000 // 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.save('mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
